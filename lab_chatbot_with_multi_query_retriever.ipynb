{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFcZFFZQx4ka"
      },
      "source": [
        "# Question Answering with LangChain, OpenAI, and MultiQuery Retriever\n",
        "\n",
        "This interactive workbook demonstrates example of Elasticsearch's [MultiQuery Retriever](https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.multi_query.MultiQueryRetriever.html) to generate similar queries for a given user input and apply all queries to retrieve a larger set of relevant documents from a vectorstore.\n",
        "\n",
        "Before we begin, we first split the fictional workplace documents into passages with `langchain` and uses OpenAI to transform these passages into embeddings and then store these into Elasticsearch.\n",
        "\n",
        "We will then ask a question, generate similar questions using langchain and OpenAI, retrieve relevant passages from the vector store, and use langchain and OpenAI again to provide a summary for the questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMyrM0d3x4kb"
      },
      "source": [
        "## Install packages and import modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsBGM_aJx4kc",
        "outputId": "2b33e98d-b4ed-40e4-970f-b2f3c9cf15dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: langchain 0.3.27\n",
            "Uninstalling langchain-0.3.27:\n",
            "  Successfully uninstalled langchain-0.3.27\n",
            "Found existing installation: langchain-core 1.0.7\n",
            "Uninstalling langchain-core-1.0.7:\n",
            "  Successfully uninstalled langchain-core-1.0.7\n",
            "Found existing installation: langchain-community 0.4.1\n",
            "Uninstalling langchain-community-0.4.1:\n",
            "  Successfully uninstalled langchain-community-0.4.1\n",
            "Found existing installation: langchain-text-splitters 1.0.0\n",
            "Uninstalling langchain-text-splitters-1.0.0:\n",
            "  Successfully uninstalled langchain-text-splitters-1.0.0\n",
            "Found existing installation: langchain-openai 0.3.35\n",
            "Uninstalling langchain-openai-0.3.35:\n",
            "  Successfully uninstalled langchain-openai-0.3.35\n",
            "Found existing installation: langchain-elasticsearch 0.4.0\n",
            "Uninstalling langchain-elasticsearch-0.4.0:\n",
            "  Successfully uninstalled langchain-elasticsearch-0.4.0\n",
            "\u001b[33mWARNING: Skipping langgraph-prebuilt as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: langchain-classic 1.0.0\n",
            "Uninstalling langchain-classic-1.0.0:\n",
            "  Successfully uninstalled langchain-classic-1.0.0\n",
            "Found existing installation: tiktoken 0.12.0\n",
            "Uninstalling tiktoken-0.12.0:\n",
            "  Successfully uninstalled tiktoken-0.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langgraph 1.0.3 requires langgraph-prebuilt<1.1.0,>=1.0.2, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# 1. CLEANUP: Uninstalls all common LangChain components and related libraries.\n",
        "# This prevents residual packages from causing dependency conflicts.\n",
        "!python3 -m pip uninstall -y langchain langchain-core langchain-community langchain-text-splitters langchain_openai langchain-elasticsearch langgraph-prebuilt langchain-classic tiktoken\n",
        "\n",
        "# 2. INSTALLATION: Installs all required base, core, and specialized component packages.\n",
        "# We explicitly include langchain and langchain-core for version stability.\n",
        "!python3 -m pip install -qU \\\n",
        "    jq \\\n",
        "    lark \\\n",
        "    tiktoken \\\n",
        "    langchain \\\n",
        "    langchain-core \\\n",
        "    langchain-community \\\n",
        "    langchain-text-splitters \\\n",
        "    langchain-openai \\\n",
        "    langchain-elasticsearch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4okMr8ALx4kc"
      },
      "source": [
        "## Connect to Elasticsearch\n",
        "\n",
        "ℹ️ We're using an Elastic Cloud deployment of Elasticsearch for this notebook. If you don't have an Elastic Cloud deployment, sign up [here](https://cloud.elastic.co/registration?utm_source=github&utm_content=elasticsearch-labs-notebook) for a free trial.\n",
        "\n",
        "We'll use the **Cloud ID** to identify our deployment, because we are using Elastic Cloud deployment. To find the Cloud ID for your deployment, go to https://cloud.elastic.co/deployments and select your deployment.\n",
        "\n",
        "We will use [ElasticsearchStore](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.elasticsearch.ElasticsearchStore.html) to connect to our elastic cloud deployment, This would help create and index data easily.  We would also send list of documents that we created in the previous step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ynPw-mc2x4kd"
      },
      "outputs": [],
      "source": [
        "# Set credentials directly (for demonstration only—never expose in shared code!)\n",
        "ELASTIC_CLOUD_ID = \"roland2:dXMtY2VudHJhbDEuZ2NwLmNsb3VkLmVzLmlvOjQ0MyQ2OGI0ZTE3ZWEwMmU0ZjVlYjMyNTcwNmMyMjU5YjcyNCRlYTAwOWJlYjZjYmE0OTQ0OWFkYWI3ZjhhMDMzNDNhMg==\"\n",
        "ELASTIC_API_KEY = \"SndMQW9wb0JybXhhNEVOc3hlZTc6UEFBX0ZieXduMkZFMmhoLV9qNmdXQQ==\"\n",
        "OPENAI_API_KEY = \"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\"\n",
        "\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "from langchain_elasticsearch import ElasticsearchStore\n",
        "\n",
        "# --- ADDED IMPORT FOR THE LLM ---\n",
        "from langchain_openai.llms import OpenAI\n",
        "# --- END ADDED IMPORT ---\n",
        "\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
        "\n",
        "vectorstore = ElasticsearchStore(\n",
        "    es_cloud_id=ELASTIC_CLOUD_ID,\n",
        "    es_api_key=ELASTIC_API_KEY,\n",
        "    index_name=\"roland_index\",\n",
        "    embedding=embeddings,\n",
        ")\n",
        "\n",
        "# Now it can find OpenAI\n",
        "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
        "\n",
        "# MultiQueryRetriever still needs to be imported, but we'll put it in the next cell (Cell 21)\n",
        "# where it's actually used, to keep cells clean."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "018mfJzwx4kd"
      },
      "source": [
        "## Indexing Data into Elasticsearch\n",
        "Let's download the sample dataset and deserialize the document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OagSVDo5x4kd"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlopen\n",
        "import json\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/elastic/elasticsearch-labs/main/example-apps/chatbot-rag-app/data/data.json\"\n",
        "\n",
        "response = urlopen(url)\n",
        "data = json.load(response)\n",
        "\n",
        "with open(\"temp.json\", \"w\") as json_file:\n",
        "    json.dump(data, json_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkFmgRPtx4kd"
      },
      "source": [
        "### Split Documents into Passages\n",
        "\n",
        "We’ll chunk documents into passages in order to improve the retrieval specificity and to ensure that we can provide multiple passages within the context window of the final question answering prompt.\n",
        "\n",
        "Here we are chunking documents into 800 token passages with an overlap of 400 tokens.\n",
        "\n",
        "Here we are using a simple splitter but Langchain offers more advanced splitters to reduce the chance of context being lost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "26mxYcITx4kd"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import JSONLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "def metadata_func(record: dict, metadata: dict) -> dict:\n",
        "    #Populate the metadata dictionary with keys name, summary, url, category, and updated_at.\n",
        "    metadata[\"name\"] = record.get(\"name\")\n",
        "    metadata[\"summary\"] = record.get(\"summary\")\n",
        "    metadata[\"url\"] = record.get(\"url\")\n",
        "    metadata[\"category\"] = record.get(\"category\")\n",
        "    metadata[\"updated_at\"] = record.get(\"updated_at\")\n",
        "\n",
        "    return metadata\n",
        "\n",
        "\n",
        "# For more loaders https://python.langchain.com/docs/modules/data_connection/document_loaders/\n",
        "# And 3rd party loaders https://python.langchain.com/docs/modules/data_connection/document_loaders/#third-party-loaders\n",
        "loader = JSONLoader(\n",
        "    file_path=\"temp.json\",\n",
        "    jq_schema=\".[]\",\n",
        "    content_key=\"content\",\n",
        "    metadata_func=metadata_func,\n",
        ")\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=800, chunk_overlap=400 #define chunk size and chunk overlap\n",
        ")\n",
        "docs = loader.load_and_split(text_splitter=text_splitter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2TroZXsx4kd"
      },
      "source": [
        "### Bulk Import Passages\n",
        "\n",
        "Now that we have split each document into the chunk size of 800, we will now index data to elasticsearch using [ElasticsearchStore.from_documents](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.elasticsearch.ElasticsearchStore.html#langchain.vectorstores.elasticsearch.ElasticsearchStore.from_documents).\n",
        "\n",
        "We will use Cloud ID, Password and Index name values set in the `Create cloud deployment` step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EsQE6HA_x4ke"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "documents = vectorstore.from_documents(\n",
        "    docs,\n",
        "    embeddings,\n",
        "    index_name=\"roland_index\",  # Changed 'roland index' to 'roland_index'\n",
        "    es_cloud_id=ELASTIC_CLOUD_ID,\n",
        "    es_api_key=ELASTIC_API_KEY,\n",
        ")\n",
        "\n",
        "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
        "\n",
        "retriever = MultiQueryRetriever.from_llm(vectorstore.as_retriever(), llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYV6Zfyex4ke"
      },
      "source": [
        "# Question Answering with MultiQuery Retriever\n",
        "\n",
        "Now that we have the passages stored in Elasticsearch, we can now ask a question to get the relevant passages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOTPtXEAx4ke",
        "outputId": "91efaf60-462d-4fdb-bb85-eea8fec897d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Can you provide information on the sales team at NASA?', '2. How does the sales team at NASA operate?', '3. What are the responsibilities of the sales team at NASA?']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- Answer ----\n",
            "The NASA sales team is a part of the Americas region in the sales organization. It is led by two Area Vice-Presidents, Laura Martinez for North America and Gary Johnson for South America. The team is responsible for serving customers in the United States, Canada, Mexico, Central and South America. They work closely with other departments to identify new business opportunities, maintain existing client relationships, and ensure customer satisfaction.\n"
          ]
        }
      ],
      "source": [
        "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough\n",
        "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from langchain.schema import format_document\n",
        "\n",
        "import logging\n",
        "\n",
        "logging.basicConfig()\n",
        "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
        "\n",
        "LLM_CONTEXT_PROMPT = ChatPromptTemplate.from_template(\n",
        "    \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Be as verbose and educational in your response as possible.\n",
        "\n",
        "    context: {context}\n",
        "    Question: \"{question}\"\n",
        "    Answer:\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "LLM_DOCUMENT_PROMPT = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "---\n",
        "SOURCE: {name}\n",
        "{page_content}\n",
        "---\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "\n",
        "def _combine_documents(\n",
        "    docs, document_prompt=LLM_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
        "):\n",
        "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
        "    return document_separator.join(doc_strings)\n",
        "\n",
        "\n",
        "_context = RunnableParallel(\n",
        "    context=retriever | _combine_documents,\n",
        "    question=RunnablePassthrough(),\n",
        ")\n",
        "\n",
        "chain = _context | LLM_CONTEXT_PROMPT | llm\n",
        "\n",
        "ans = chain.invoke(\"what is the nasa sales team?\")\n",
        "\n",
        "print(\"---- Answer ----\")\n",
        "print(ans)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMW789Zux4ke"
      },
      "source": [
        "**Generate at least two new iteratioins of the previous cells - Be creative.** Did you master Multi-\n",
        "Query Retriever concepts through this lab?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SUMMARY: Tests the MultiQuery Retriever with a complex question that requires\n",
        "# the LLM to generate multiple sub-queries (e.g., 'What is the policy?',\n",
        "# 'Who is the contact?', 'What's the official name?') to gather context from\n",
        "# different parts of the knowledge base before synthesizing a single answer.\n",
        "\n",
        "complex_query = \"What is the process for submitting a bug report for the WonderVector5000, and who is the engineering lead for that product?\"\n",
        "\n",
        "ans_2 = chain.invoke(complex_query)\n",
        "\n",
        "print(\"--- Query ---\")\n",
        "print(complex_query)\n",
        "print(\"\\n---- Answer ----\")\n",
        "print(ans_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WK7ilhxoFABC",
        "outputId": "8a6db130-a0d3-4f8b-e8ab-5c6040fa8d6b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. How can I submit a bug report for the WonderVector5000? Who is the engineering lead for this product?', '2. What are the steps to follow when submitting a bug report for the WonderVector5000? Can you tell me who the engineering lead is for this product?', \"3. Is there a specific process for reporting bugs related to the WonderVector5000? I'm also curious about the engineering lead for this product.\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Query ---\n",
            "What is the process for submitting a bug report for the WonderVector5000, and who is the engineering lead for that product?\n",
            "\n",
            "---- Answer ----\n",
            "\n",
            "To submit a bug report for the WonderVector5000, you would first need to identify the issue and gather any relevant information, such as steps to reproduce the bug and any error messages. Then, you would need to log into the bug tracking system and create a new bug report, providing all the necessary details. This report would then be assigned to the engineering team responsible for the WonderVector5000.\n",
            "\n",
            "The engineering lead for the WonderVector5000 would be the Principal Software Engineer, as they are responsible for leading the design, development, and maintenance of large-scale, mission-critical software applications and components. They also provide technical leadership and mentorship to software engineering teams, making them the point person for any issues related to the WonderVector5000.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SUMMARY: This notebook demonstrates the MultiQuery Retriever, which is a technique\n",
        "# that allows an LLM to generate multiple distinct search queries (thoughts) from a\n",
        "# single complex user question to retrieve a wider and more relevant context from the\n",
        "# Elasticsearch vector store before synthesizing a final, grounded answer.\n",
        "\n",
        "complex_query_2 = \"What is the work from home policy?\"\n",
        "\n",
        "ans_3 = chain.invoke(complex_query_2)\n",
        "\n",
        "print(\"--- Query ---\")\n",
        "print(complex_query_2)\n",
        "print(\"\\n---- Answer ----\")\n",
        "print(ans_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGMj9TGAFqOH",
        "outputId": "52baeb2d-928f-44b9-e6f8-b381da478ba4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: [\"1. Can you provide information on the company's work from home policy?\", '2. How does the work from home policy at this company compare to others in the industry?', '3. What are the guidelines and restrictions of the work from home policy in place?']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Query ---\n",
            "What is the work from home policy?\n",
            "\n",
            "---- Answer ----\n",
            "\n",
            "The work from home policy is a set of guidelines and support provided to employees to conduct their work remotely. It was implemented in March 2020 in response to the COVID-19 pandemic and is designed to ensure the continuity and productivity of business operations. The policy applies to all eligible employees and allows them to work from home full-time while maintaining the same level of performance and collaboration as they would in the office. Employees must have approval from their direct supervisor and the HR department to be eligible for this arrangement. The company provides necessary equipment and resources for remote work, and employees are responsible for creating a comfortable and safe workspace. Effective communication, maintaining regular work hours and availability, and meeting performance expectations are also important aspects of the policy. Employees are required to accurately track their work hours and adhere to confidentiality and data security policies. The company also encourages employees to prioritize their health and well-being while working from home. The policy will be periodically reviewed and updated as necessary, and employees can direct any questions or concerns to their supervisor or the HR department.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SUMMARY: This notebook demonstrates the MultiQuery Retriever, which is a technique\n",
        "# that allows an LLM to generate multiple distinct search queries (thoughts) from a\n",
        "# single complex user question to retrieve a wider and more relevant context from the\n",
        "# Elasticsearch vector store before synthesizing a final, grounded answer.\n",
        "\n",
        "complex_query_3 = \"What is the sales strategy for fiscal year 2024?\"\n",
        "\n",
        "ans_4 = chain.invoke(complex_query_3)\n",
        "\n",
        "print(\"--- Query ---\")\n",
        "print(complex_query_3)\n",
        "print(\"\\n---- Answer ----\")\n",
        "print(ans_4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_yyf-ZgGR_q",
        "outputId": "fc517761-96a5-4763-9b10-97a4319cbad9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What are the key components of the sales strategy for fiscal year 2024?', '2. How does the sales strategy for fiscal year 2024 differ from previous years?', '3. Can you provide insights into the sales strategy for fiscal year 2024 and its potential impact on revenue?']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Query ---\n",
            "What is the sales strategy for fiscal year 2024?\n",
            "\n",
            "---- Answer ----\n",
            "\n",
            "The sales strategy for fiscal year 2024 is outlined in a document that includes key objectives, focus areas, and action plans for a tech company's sales operations. The primary goal of this strategy is to increase revenue, expand market share, and strengthen customer relationships in the company's target markets. The specific objectives for fiscal year 2024 include increasing revenue by 20%, expanding market share in key segments by 15%, retaining 95% of existing customers, and launching at least two new products or services in high-demand market segments. The focus areas of the strategy include targeting high-growth industries in existing markets, identifying and penetrating new markets, strengthening relationships with key accounts and strategic partners, pursuing new customers in underserved market segments, and developing tailored offerings for different customer segments. The action plans for achieving these objectives include expanding the sales team, providing ongoing training to sales staff, implementing a performance-based incentive system, developing targeted marketing campaigns, leveraging digital marketing channels, participating in industry events and trade shows, strengthening partnerships, implementing a proactive customer success program, and conducting regular performance reviews. The strategy also includes monitoring and evaluation through the establishment of key performance indicators and quarterly reviews to ensure alignment with market trends. By following this sales strategy, the tech company aims to achieve significant\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}